{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build the CNN as described in our reference paper by Xu and Zhou (2020). We choose to implement the model in TensorFlow / Keras as this is the Deep Learning library we are the most comfortable with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import List, Union"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Squeeze-Excitation block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SqueezeLayer, self).__init__()\n",
    "    \n",
    "    def call(self, input_tensor):\n",
    "        # channel-wise average\n",
    "        return tf.einsum(\"...ijk->...k\", input_tensor) / (input_tensor.get_shape()[-3] * input_tensor.get_shape()[-2])\n",
    "\n",
    "class SqueezeExcitationBlock(tf.keras.Model):\n",
    "    def __init__(self, reduction_ratio: float, name=None):\n",
    "        super(SqueezeExcitationBlock, self).__init__(name=name)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "    def build(self, input_tensor):\n",
    "        n_channels = input_tensor.get_shape()[-1]\n",
    "        \n",
    "        self.squeeze_layer = SqueezeLayer()\n",
    "\n",
    "        # hidden layer\n",
    "        self.layer1 = tf.keras.layers.Dense(round(n_channels * self.reduction_ratio), activation=tf.keras.activations.relu, use_bias=False)\n",
    "\n",
    "        # importance weights\n",
    "        self.layer2 = tf.keras.layers.Dense(n_channels, activation=tf.keras.activations.sigmoid, use_bias=False)\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        # squeeze\n",
    "        w = self.squeeze_layer(input_tensor)\n",
    "\n",
    "        # excitation\n",
    "        w = self.layer1(w)\n",
    "        w = self.layer2(w)\n",
    "\n",
    "        # channel-wise multiplication\n",
    "        return tf.einsum(\"...k,...ijk->...ijk\", w, input_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define convolutional block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, a dropout layer was added after each convolutional block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvolutionalBlock(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size=3, padding='same', dropout_rate=0.25, name=None):\n",
    "        super(ConvolutionalBlock, self).__init__(name=name)\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding=padding)\n",
    "        self.max_pool = tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='valid')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        x = self.conv(input_tensor)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcitationCNN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_blocks: int = 5,\n",
    "        filters: List[int] = [32, 64, 128, 256, 128],\n",
    "        kernel_size: Union[List[int], int] = 3,\n",
    "        padding: Union[List[str], str] = 'same',\n",
    "        dropout_rate: Union[List[int], int] = 0.25,\n",
    "        pooling: str = 'flatten',  # or 'gap',\n",
    "        return_features: bool = False,  # if True, returns the feature vector, not the softmax output\n",
    "        n_genres: int = 10,\n",
    "        name=None\n",
    "    ):\n",
    "        super(SqueezeExcitationCNN, self).__init__(name=name)\n",
    "        try:\n",
    "            assert ...# All same length = n blocks\n",
    "        except AssertionError as err:\n",
    "            print(\"All parameters should be scalars or lists of length equal to the number of blocks\")\n",
    "            raise err.with_traceback()\n",
    "\n",
    "        self.filters = filters\n",
    "        ...\n",
    "\n",
    "    def build(self, input_tensor):\n",
    "        x = input_tensor\n",
    "\n",
    "        # Alternating Conv and SE blocks\n",
    "        self.conv_blocks = []\n",
    "        self.se_blocks = []\n",
    "        for k in range(self.n_blocks):\n",
    "            self.conv_blocks.append(\n",
    "                ConvolutionalBlock(\n",
    "                    filters=self.filters[k],\n",
    "                    kernel_size=self.kernel_size[k],\n",
    "                    padding=self.padding[k],\n",
    "                    dropout_rate=self.dropout_rate[k],\n",
    "                    name=(self.name + \"_ConvBlock%d\" % k)\n",
    "                )\n",
    "            )\n",
    "            self.se_blocks.append(\n",
    "                SqueezeExcitationBlock(\n",
    "                    reduction_ratio=self.reduction_ratios[k],\n",
    "                    name=(self.name + \"_SE_Block%d\" % k)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if not self.return_features:\n",
    "            # classifier as decribed in the paper\n",
    "            self.classifier = tf.keras.Dense(self.n_genres, activation=tf.keras.activations.softmax)\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        x = input_tensor\n",
    "\n",
    "        # alternating conv and se blocks\n",
    "        for conv, se in zip(self.conv_blocks, self.se_blocks):\n",
    "            x = conv(x)\n",
    "            x = se(x)\n",
    "\n",
    "        # flatten or gap to feature vector\n",
    "        x = self.vectorize(x)\n",
    "\n",
    "        if self.return_features:\n",
    "            return x\n",
    "        else:\n",
    "            return self.classifier(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth, number of neurons, number of SE blocks (ref paper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate, optimizer algorithm, ways to mitigate overfitting (regularization, dropout)\n",
    "Batch norm (should it be in model architecture ?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction ratio in SE Blocks\n",
    "In the original paper they define r such that in the SE block the weight matrices are of shape C/r x C\n",
    "After bayesian optimization, with search space for r being [8, 32], they settle on an optimal value of 31.43 (so basically, 32?)\n",
    "Apparently, they settled on a unique value for the reduction ratio, whereas we could have different reduction ratio values for the 5 different SE blocks.\n",
    "Furthermore, in their definition of the reduction ratio, the ratio does not have the same effect depending on the SE block (it reduces the number of channels in the first SE block to 1, 2 in the 2nd channel, 4 in the 3rd, then 8 and 4)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NAML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4dcf0feba9779ca3a67ecd85ea7c5955efb41798f88d0e943b148aee3967434f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
